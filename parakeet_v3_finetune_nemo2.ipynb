{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Parakeet-v3 ASR Model with NeMo 2.5+\n",
    "\n",
    "This notebook demonstrates how to fine-tune the NVIDIA Parakeet-v3 (parakeet-tdt-0.6b-v3) ASR model using NeMo 2.5+.\n",
    "\n",
    "## Key Updates from Original Tutorial:\n",
    "- Updated to work with NeMo 2.5+ (from 1.23)\n",
    "- Uses Parakeet-v3 model instead of original Parakeet\n",
    "- Compatible with Modal GPU infrastructure for training\n",
    "- Updated API calls and configuration structure\n",
    "\n",
    "## Model Information:\n",
    "- **Model**: nvidia/parakeet-tdt-0.6b-v3\n",
    "- **Architecture**: FastConformer-TDT\n",
    "- **Parameters**: 600M\n",
    "- **Languages**: 25 European languages\n",
    "- **License**: CC BY 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update && apt-get install -y sox libsndfile1 ffmpeg libsox-fmt-mp3 jq wget\n",
    "\n",
    "# Install Python dependencies\n",
    "!pip install text-unidecode matplotlib>=3.3.2 Cython librosa soundfile\n",
    "!pip install huggingface-hub>=0.23.2\n",
    "\n",
    "# Install NeMo 2.5+ (latest version)\n",
    "!pip install nemo_toolkit[asr]>=2.5.0\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import glob\n",
    "import subprocess\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# NeMo imports for v2.5+\n",
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.models import ASRModel\n",
    "from nemo.core.config import hydra_runner\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "# Check NeMo version\n",
    "print(f\"NeMo version: {nemo.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We'll use the AN4 dataset for demonstration, but you can replace this with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data directories\n",
    "DATA_DIR = os.getcwd()\n",
    "os.environ[\"DATA_DIR\"] = DATA_DIR\n",
    "\n",
    "# Download AN4 dataset\n",
    "if not os.path.exists(f\"{DATA_DIR}/an4_sphere.tar.gz\"):\n",
    "    !wget https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz\n",
    "    \n",
    "# Extract dataset\n",
    "if not os.path.exists(f\"{DATA_DIR}/an4\"):\n",
    "    !tar -xvf an4_sphere.tar.gz\n",
    "    !mv an4 $DATA_DIR\n",
    "\n",
    "print(\"Dataset downloaded and extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def an4_build_manifest(transcripts_path, manifest_path, target_wavs_dir):\n",
    "    \"\"\"Build an AN4 manifest from a given transcript file.\"\"\"\n",
    "    with open(transcripts_path, 'r') as fin:\n",
    "        with open(manifest_path, 'w') as fout:\n",
    "            for line in fin:\n",
    "                # Lines look like this:\n",
    "                # <s> transcript </s> (fileID)\n",
    "                transcript = line[: line.find('(') - 1].lower()\n",
    "                transcript = transcript.replace('<s>', '').replace('</s>', '')\n",
    "                transcript = transcript.strip()\n",
    "\n",
    "                file_id = line[line.find('(') + 1 : -2]  # e.g. \"cen4-fash-b\"\n",
    "                audio_path = os.path.join(target_wavs_dir, file_id + '.wav')\n",
    "\n",
    "                if os.path.exists(audio_path):\n",
    "                    duration = librosa.core.get_duration(filename=audio_path)\n",
    "                    # Write the metadata to the manifest\n",
    "                    metadata = {\"audio_filepath\": audio_path, \"duration\": duration, \"text\": transcript}\n",
    "                    json.dump(metadata, fout)\n",
    "                    fout.write('\\n')\n",
    "\n",
    "# Process AN4 dataset\n",
    "source_data_dir = f\"{DATA_DIR}/an4\"\n",
    "target_data_dir = f\"{DATA_DIR}/an4_converted\"\n",
    "\n",
    "if not os.path.exists(source_data_dir):\n",
    "    raise ValueError(f\"Data not found at `{source_data_dir}`. Please ensure the AN4 dataset is properly extracted.\")\n",
    "\n",
    "# Convert SPH files to WAV files\n",
    "sph_list = glob.glob(os.path.join(source_data_dir, '**/*.sph'), recursive=True)\n",
    "target_wavs_dir = os.path.join(target_data_dir, 'wavs')\n",
    "\n",
    "if not os.path.exists(target_wavs_dir):\n",
    "    print(f\"Creating directories for {target_wavs_dir}.\")\n",
    "    os.makedirs(target_wavs_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Converting {len(sph_list)} SPH files to WAV...\")\n",
    "for sph_path in sph_list:\n",
    "    wav_path = os.path.join(target_wavs_dir, os.path.splitext(os.path.basename(sph_path))[0] + '.wav')\n",
    "    if not os.path.exists(wav_path):\n",
    "        cmd = [\"sox\", sph_path, wav_path]\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "# Build AN4 manifests\n",
    "train_transcripts = os.path.join(source_data_dir, 'etc/an4_train.transcription')\n",
    "train_manifest = os.path.join(target_data_dir, 'train_manifest.json')\n",
    "an4_build_manifest(train_transcripts, train_manifest, target_wavs_dir)\n",
    "\n",
    "test_transcripts = os.path.join(source_data_dir, 'etc/an4_test.transcription')\n",
    "test_manifest = os.path.join(target_data_dir, 'test_manifest.json')\n",
    "an4_build_manifest(test_transcripts, test_manifest, target_wavs_dir)\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n",
    "print(f\"Train manifest: {train_manifest}\")\n",
    "print(f\"Test manifest: {test_manifest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Parakeet-v3 Model\n",
    "\n",
    "We'll load the pre-trained Parakeet-v3 model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Parakeet-v3 model\n",
    "model_name = \"nvidia/parakeet-tdt-0.6b-v3\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_name)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model type: {type(asr_model)}\")\n",
    "print(f\"Model architecture: {asr_model.__class__.__name__}\")\n",
    "\n",
    "# Display model configuration\n",
    "print(\"\\nModel configuration:\")\n",
    "print(f\"Encoder: {asr_model.encoder.__class__.__name__}\")\n",
    "print(f\"Decoder: {asr_model.decoder.__class__.__name__}\")\n",
    "print(f\"Vocabulary size: {asr_model.decoder.vocab_size if hasattr(asr_model.decoder, 'vocab_size') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Pre-trained Model\n",
    "\n",
    "Let's test the pre-trained model on a sample audio file before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample audio file for testing\n",
    "sample_audio = \"2086-149220-0033.wav\"\n",
    "if not os.path.exists(sample_audio):\n",
    "    !wget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\n",
    "\n",
    "# Test transcription\n",
    "print(\"Testing pre-trained model...\")\n",
    "output = asr_model.transcribe([sample_audio])\n",
    "print(f\"Transcription: {output[0].text}\")\n",
    "\n",
    "# Test with timestamps\n",
    "print(\"\\nTesting with timestamps...\")\n",
    "output_with_timestamps = asr_model.transcribe([sample_audio], timestamps=True)\n",
    "if hasattr(output_with_timestamps[0], 'timestamp') and output_with_timestamps[0].timestamp:\n",
    "    word_timestamps = output_with_timestamps[0].timestamp.get('word', [])\n",
    "    print(\"Word-level timestamps:\")\n",
    "    for stamp in word_timestamps[:5]:  # Show first 5 words\n",
    "        print(f\"  {stamp['start']:.2f}s - {stamp['end']:.2f}s : {stamp['word']}\")\n",
    "else:\n",
    "    print(\"Timestamps not available in this output format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning Configuration\n",
    "\n",
    "Set up the configuration for fine-tuning with NeMo 2.5+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "import pytorch_lightning as pl\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "# Create fine-tuning configuration\n",
    "def create_finetune_config():\n",
    "    config = OmegaConf.create({\n",
    "        'model': {\n",
    "            'train_ds': {\n",
    "                'manifest_filepath': train_manifest,\n",
    "                'sample_rate': 16000,\n",
    "                'batch_size': 8,  # Adjust based on GPU memory\n",
    "                'shuffle': True,\n",
    "                'num_workers': 4,\n",
    "                'pin_memory': True,\n",
    "                'use_start_end_token': False,\n",
    "            },\n",
    "            'validation_ds': {\n",
    "                'manifest_filepath': test_manifest,\n",
    "                'sample_rate': 16000,\n",
    "                'batch_size': 8,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 4,\n",
    "                'pin_memory': True,\n",
    "                'use_start_end_token': False,\n",
    "            },\n",
    "            'optim': {\n",
    "                'name': 'adamw',\n",
    "                'lr': 1e-4,  # Lower learning rate for fine-tuning\n",
    "                'weight_decay': 0.001,\n",
    "                'sched': {\n",
    "                    'name': 'CosineAnnealing',\n",
    "                    'warmup_steps': 100,\n",
    "                    'min_lr': 1e-6,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'trainer': {\n",
    "            'devices': 1,\n",
    "            'max_epochs': 10,  # Adjust as needed\n",
    "            'precision': 'bf16-mixed' if torch.cuda.is_available() else 32,\n",
    "            'accelerator': 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "            'strategy': 'auto',\n",
    "            'enable_checkpointing': True,\n",
    "            'logger': True,\n",
    "            'log_every_n_steps': 10,\n",
    "            'val_check_interval': 1.0,\n",
    "            'gradient_clip_val': 1.0,\n",
    "        },\n",
    "        'exp_manager': {\n",
    "            'exp_dir': f'{DATA_DIR}/checkpoints',\n",
    "            'name': 'parakeet_v3_finetune',\n",
    "            'version': 'v1',\n",
    "            'use_datetime_version': False,\n",
    "            'create_tensorboard_logger': True,\n",
    "            'create_checkpoint_callback': True,\n",
    "            'checkpoint_callback_params': {\n",
    "                'monitor': 'val_wer',\n",
    "                'mode': 'min',\n",
    "                'save_top_k': 3,\n",
    "                'save_last': True,\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    return config\n",
    "\n",
    "# Create configuration\n",
    "cfg = create_finetune_config()\n",
    "print(\"Fine-tuning configuration created!\")\n",
    "print(f\"Training manifest: {cfg.model.train_ds.manifest_filepath}\")\n",
    "print(f\"Validation manifest: {cfg.model.validation_ds.manifest_filepath}\")\n",
    "print(f\"Max epochs: {cfg.trainer.max_epochs}\")\n",
    "print(f\"Learning rate: {cfg.model.optim.lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning Process\n",
    "\n",
    "Now we'll fine-tune the Parakeet-v3 model on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup experiment manager\n",
    "trainer = pl.Trainer(**cfg.trainer)\n",
    "exp_dir = exp_manager(trainer, cfg.exp_manager)\n",
    "\n",
    "# Update model configuration for fine-tuning\n",
    "asr_model.set_trainer(trainer)\n",
    "\n",
    "# Setup data loaders\n",
    "asr_model.setup_training_data(cfg.model.train_ds)\n",
    "asr_model.setup_validation_data(cfg.model.validation_ds)\n",
    "\n",
    "# Configure optimizer\n",
    "asr_model.configure_optimizers()\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(f\"Experiment directory: {exp_dir}\")\n",
    "\n",
    "# Start training\n",
    "trainer.fit(asr_model)\n",
    "\n",
    "print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best checkpoint\n",
    "checkpoint_dir = f\"{exp_dir}/checkpoints\"\n",
    "checkpoint_files = glob.glob(f\"{checkpoint_dir}/*.ckpt\")\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Find the best checkpoint (lowest validation WER)\n",
    "    best_checkpoint = None\n",
    "    for ckpt in checkpoint_files:\n",
    "        if \"last\" not in ckpt:  # Skip last.ckpt, look for best\n",
    "            best_checkpoint = ckpt\n",
    "            break\n",
    "    \n",
    "    if best_checkpoint is None:\n",
    "        best_checkpoint = checkpoint_files[0]  # Use any available checkpoint\n",
    "    \n",
    "    print(f\"Loading checkpoint: {best_checkpoint}\")\n",
    "    \n",
    "    # Load the fine-tuned model\n",
    "    finetuned_model = ASRModel.load_from_checkpoint(best_checkpoint)\n",
    "    \n",
    "    # Test the fine-tuned model\n",
    "    print(\"\\nTesting fine-tuned model...\")\n",
    "    output_finetuned = finetuned_model.transcribe([sample_audio])\n",
    "    print(f\"Fine-tuned transcription: {output_finetuned[0].text}\")\n",
    "    \n",
    "    # Compare with original\n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"Original model: {output[0].text}\")\n",
    "    print(f\"Fine-tuned model: {output_finetuned[0].text}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No checkpoints found. Using the current model state.\")\n",
    "    finetuned_model = asr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Export\n",
    "\n",
    "Export the fine-tuned model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model in NeMo format\n",
    "output_model_path = f\"{DATA_DIR}/parakeet_v3_finetuned.nemo\"\n",
    "\n",
    "if 'finetuned_model' in locals():\n",
    "    finetuned_model.save_to(output_model_path)\n",
    "    print(f\"Fine-tuned model saved to: {output_model_path}\")\n",
    "    \n",
    "    # Verify the saved model can be loaded\n",
    "    print(\"\\nVerifying saved model...\")\n",
    "    loaded_model = ASRModel.restore_from(output_model_path)\n",
    "    test_output = loaded_model.transcribe([sample_audio])\n",
    "    print(f\"Loaded model transcription: {test_output[0].text}\")\n",
    "    print(\"Model verification successful!\")\n",
    "    \n",
    "else:\n",
    "    print(\"No fine-tuned model available to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Evaluation (Optional)\n",
    "\n",
    "Evaluate the model on the entire test set to compute WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch evaluation on test set\n",
    "if 'finetuned_model' in locals():\n",
    "    print(\"Running batch evaluation on test set...\")\n",
    "    \n",
    "    # Read test manifest\n",
    "    test_files = []\n",
    "    test_texts = []\n",
    "    \n",
    "    with open(test_manifest, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            test_files.append(data['audio_filepath'])\n",
    "            test_texts.append(data['text'])\n",
    "    \n",
    "    print(f\"Evaluating on {len(test_files)} test files...\")\n",
    "    \n",
    "    # Transcribe all test files\n",
    "    predictions = finetuned_model.transcribe(test_files[:10])  # Limit to first 10 for demo\n",
    "    \n",
    "    # Display some results\n",
    "    print(\"\\nSample results:\")\n",
    "    for i, (pred, true_text) in enumerate(zip(predictions[:5], test_texts[:5])):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  Ground truth: {true_text}\")\n",
    "        print(f\"  Prediction:   {pred.text}\")\n",
    "    \n",
    "    print(\"\\nBatch evaluation completed!\")\n",
    "else:\n",
    "    print(\"No fine-tuned model available for batch evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Set up NeMo 2.5+** environment with all required dependencies\n",
    "2. **Load Parakeet-v3** model from HuggingFace\n",
    "3. **Prepare training data** in the correct format\n",
    "4. **Fine-tune the model** on custom data\n",
    "5. **Evaluate and export** the fine-tuned model\n",
    "\n",
    "### Key Differences from NeMo 1.23:\n",
    "- Updated import statements and API calls\n",
    "- New configuration structure with OmegaConf\n",
    "- Updated trainer and experiment manager setup\n",
    "- Improved model loading from HuggingFace\n",
    "\n",
    "### Next Steps:\n",
    "1. **Scale up training** with more epochs and larger datasets\n",
    "2. **Experiment with hyperparameters** (learning rate, batch size, etc.)\n",
    "3. **Deploy the model** using NVIDIA Riva or other inference frameworks\n",
    "4. **Evaluate on domain-specific data** for your use case\n",
    "\n",
    "### For Production Use:\n",
    "- Use larger batch sizes and multiple GPUs for faster training\n",
    "- Implement proper validation and early stopping\n",
    "- Add comprehensive logging and monitoring\n",
    "- Consider using distributed training for very large datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}